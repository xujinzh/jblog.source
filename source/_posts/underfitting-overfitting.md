---
title: 深度学习中的欠拟合和过拟合
mathjax: true
author: Jinzhong Xu
date: 2021-06-18 16:52:14
tags:
- ml
- dl
- cv
categories:
- research
- deep learning
---

人类专家在完成某项任务的准确性可以作为深度学习完成同样任务合理目标的参考。由于人类专家人类专家也有犯错的概率，因此，利用他们准备的数据集训练模型，不能期望100%的准确率。在深度学习模型训练中常遇到欠拟合和过拟合，我认为这主要是因为数据量和模型不匹配导致的。当模型简单时，如线性模型、参数量少等，训练数据量需求相对较少；当模型复杂时，如指数、参数量多等，训练数据量需求就会相对较多。

<!--more-->

# 欠拟合

当模型的训练误差明显大于理想模型的预期误差时，说明发生了欠拟合。在深度学习中，当模型在训练集上表现不佳时，称模型具有高偏差（bias）。

## 解决欠拟合

处理欠拟合模型的最佳方法是尝试更大的神经网络（添加新层或增加现有层中的神经元数量）或对模型进行更长时间的训练。

1. 更大的神经网络

   **这是因为欠拟合的神经网络可能不足以捕捉训练数据集中的模式。在这种情况下，添加更多层或增加神经元数量可能有助于解决问题。**

2. 更长时间的训练

   欠拟合的模型是尚未找到参数最佳值，增加训练时间可能会有帮助。

# 过拟合

当模型在训练集上表现良好，但在使用验证集或测试数据集时无法达到良好的准确性时，就会发生过拟合。这类问题称为高方差（variance），通常意味着模型无法概括训练数据集的见解。

## 解决过拟合

过拟合的最佳解决方案是获取更多数据并使用正则化。总结解决过拟合的方法如下：

1. $L_1$ 和 $L_2$ 正则化方法用于以惩罚更复杂模型的方式修改代价函数
2. Dropout 用于防止模型过分依赖一个输入，因为输入可能会消失
3. 提前停止来防止“记忆”训练样本
4. 数据增强 — 使训练数据集更加多样化

# 参考链接

1. [How to deal with underfitting and overfitting in deep learning](https://www.mikulskibartosz.name/how-to-deal-with-underfitting-and-overfitting-in-deep-learning/)

